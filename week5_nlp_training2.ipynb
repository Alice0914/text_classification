{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Learning & One-Shot Learning\n",
    "- https://www.kaggle.com/competitions/humpback-whale-identification/discussion/73454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Challenge: Traditional machine learning models require large amounts of labeled data to train effectively.  Few-shot and one-shot learning are geared towards scenarios where you have very limited labeled data per class.\n",
    "\n",
    "- **Few-Shot Learning**:  Training models to learn new categories from a few examples (typically 2-5).\n",
    "\n",
    "- **One-Shot Learning**:  An extreme case of few-shot learning where you only have a single example per new category.\n",
    "\n",
    "- Techniques:\n",
    "    - Meta-Learning: Focuses on \"learning to learn.\" The model is trained across many tasks to become good at adapting to new classes with minimal data.\n",
    "    - Data Augmentation: Generating variations of your limited examples to expand the training set.\n",
    "    - Transfer Learning: Leveraging a pre-trained model and fine-tuning it on the small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Semi-Supervised Learning a machine learning approach that combines a small amount of labeled data with a large amount of unlabeled data during training. \n",
    "- Scenario: You have a dataset with a small portion of labeled data and a much larger portion of unlabeled data.\n",
    "- Goal: Semi-supervised learning aims to leverage both the labeled and unlabeled data to improve the model's performance.\n",
    "- Techniques:\n",
    "    - Self-Training: The model trains on the labeled data, predicts labels for the unlabeled data, and then retrains on the combined set.\n",
    "    - Co-Training: Multiple models train on different aspects (or views) of the data, and their predictions are used to label unlabeled examples for each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pairwise Learning is a strategy used in machine learning where the model learns from pairs of examples.\n",
    "- It is often used in tasks like ranking, recommendation systems, or anywhere the relative comparison between two items is important. For example, in a recommendation system, pairwise learning could be used to learn user preferences by comparing pairs of items to determine which is preferred.\n",
    "- Applications:\n",
    "    - Ranking: Learning to rank items (e.g., search results).\n",
    "    - Metric Learning: Learning a distance function that meaningfully captures similarity between examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Networks, Triplet Loss, & Contrastive Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Architecture of Siamese Network\n",
    "    <div>\n",
    "    <img src = \"nlp_images/siameseNetwork.png\" width = 700>\n",
    "    </div>\n",
    "\n",
    "- **Siamese Networks**: Neural networks with two identical \"branches\" that share weights.  They take two inputs, process them, and the outputs are compared. “Identical” here means they have the same configuration with the same parameters and weights. Parameter updating is mirrored across both sub-networks and it’s used to find similarities between  inputs by comparing its feature vectors.\n",
    "- **Loss Functions**\n",
    "    - **Triplet Loss**: Used to train Siamese networks. A triplet consists of:\n",
    "        - Anchor: A reference example.\n",
    "        - Positive: An example similar to the Anchor.\n",
    "        - Negative: An example dissimilar to the Anchor.\n",
    "        - Triplet loss aims to minimize the distance between the Anchor and Positive representations while maximizing the distance between the Anchor and Negative.\n",
    "        - Triplet loss is a loss function where a baseline (anchor) input is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized.\n",
    "            <div>\n",
    "            <img src = \"nlp_images/tripletLoss.png\" width = 400>\n",
    "            </div>\n",
    "\n",
    "    - **Contrastive Loss**:  A distance-based loss function that minimizes the Euclidean distance between similar points and maximizes it between dissimilar points. It operates on pairs:\n",
    "        - Positive Pair: Two examples that are similar.\n",
    "        - Negative Pair: Two examples that are dissimilar.\n",
    "        - Contrastive loss encourages representations of positive pairs to be close together and negative pairs to be far apart.\n",
    "        - Contrastive Loss is a popular loss function used highly nowadays, It is a distance-based loss as opposed to more conventional error-prediction losses. This loss is used to learn embeddings in which two similar points have a low Euclidean distance and two dissimilar points have a large Euclidean distance.\n",
    "        <div>\n",
    "        <img src = \"nlp_images/ContrastiveLoss.png\" width = 400>\n",
    "        </div>\n",
    "\n",
    "- Signature verification with Siamese Networks:\n",
    "    <div>\n",
    "    <img src = \"nlp_images/verification_SNN.png\" width = 700>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- **Few-Shot and One-Shot Learning** focus on learning from very few examples, with applications in domains where data collection is challenging.\n",
    "- **Semi-Supervised Learning** leverages both labeled and unlabeled data to improve learning efficiency.\n",
    "- **Pairwise Learning** involves learning from pairs of examples, useful in ranking and recommendation systems.\n",
    "- **Siamese Networks (SNN) with Triplet Loss** learn embeddings by comparing an anchor to both similar and dissimilar examples.\n",
    "- **Contrastive Loss** aims to separate similar and dissimilar pairs in embedding space, beneficial for similarity comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "import evaluate\n",
    "from itertools import chain\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-art': 1,\n",
       " 'B-eve': 2,\n",
       " 'B-geo': 3,\n",
       " 'B-gpe': 4,\n",
       " 'B-nat': 5,\n",
       " 'B-org': 6,\n",
       " 'B-per': 7,\n",
       " 'B-tim': 8,\n",
       " 'I-art': 9,\n",
       " 'I-eve': 10,\n",
       " 'I-geo': 11,\n",
       " 'I-gpe': 12,\n",
       " 'I-nat': 13,\n",
       " 'I-org': 14,\n",
       " 'I-per': 15,\n",
       " 'I-tim': 16}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Label encoding\n",
    "df = pd.read_csv('TalkFile_ner_2.csv').iloc[:300,:]\n",
    "df['Tag'] = df['Tag'].apply(eval)\n",
    "list_all_tag = df.Tag.to_list()\n",
    "list_labels = ['O'] + sorted(set(chain.from_iterable(list_all_tag)) - {'O'})\n",
    "\n",
    "label2ind = {label: idx for idx, label in enumerate(list_labels)}\n",
    "ind2label = {idx: label for label, idx in label2ind.items()}\n",
    "label2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 11, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 6, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             tokens  \\\n",
       "0   0  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1   1  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2   2  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3   3  [Police, put, the, number, of, marchers, at, 1...   \n",
       "4   4  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 11, 0]  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 6, ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence Tokenization & Label Encoding\n",
    "data_dict = {\n",
    "    'id': list(range(len(df))),\n",
    "    'tokens': [sentence.split(' ') for sentence in df['Sentence']],\n",
    "    'ner_tags': [list(map(label2ind.get, tags)) for tags in df['Tag']]\n",
    "}\n",
    "new_df = pd.DataFrame(data_dict)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     anchor_id                                      anchor_tokens  \\\n",
      "0            0  [Thousands, of, demonstrators, have, marched, ...   \n",
      "1            1  [Families, of, soldiers, killed, in, the, conf...   \n",
      "2            2  [They, marched, from, the, Houses, of, Parliam...   \n",
      "3            3  [Police, put, the, number, of, marchers, at, 1...   \n",
      "4            4  [The, protest, comes, on, the, eve, of, the, a...   \n",
      "..         ...                                                ...   \n",
      "293        293  [People, who, worship, in, unauthorized, ways,...   \n",
      "294        294  [President, Bush, has, issued, 14, pardons, to...   \n",
      "295        295  [The, pardons, ,, announced, Monday, ,, includ...   \n",
      "296        296  [They, were, for, people, convicted, of, such,...   \n",
      "297        297  [Including, Monday, 's, actions, ,, Mr., Bush,...   \n",
      "\n",
      "     positive_id                                    positive_tokens  \\\n",
      "0              1  [Families, of, soldiers, killed, in, the, conf...   \n",
      "1              2  [They, marched, from, the, Houses, of, Parliam...   \n",
      "2              3  [Police, put, the, number, of, marchers, at, 1...   \n",
      "3              4  [The, protest, comes, on, the, eve, of, the, a...   \n",
      "4              5  [The, party, is, divided, over, Britain, 's, p...   \n",
      "..           ...                                                ...   \n",
      "293          294  [President, Bush, has, issued, 14, pardons, to...   \n",
      "294          295  [The, pardons, ,, announced, Monday, ,, includ...   \n",
      "295          296  [They, were, for, people, convicted, of, such,...   \n",
      "296          297  [Including, Monday, 's, actions, ,, Mr., Bush,...   \n",
      "297          298  [Pardons, are, one, of, the, president, 's, ab...   \n",
      "\n",
      "     negative_id                                    negative_tokens  \n",
      "0              2  [They, marched, from, the, Houses, of, Parliam...  \n",
      "1              3  [Police, put, the, number, of, marchers, at, 1...  \n",
      "2              4  [The, protest, comes, on, the, eve, of, the, a...  \n",
      "3              5  [The, party, is, divided, over, Britain, 's, p...  \n",
      "4              6  [The, London, march, came, ahead, of, anti-war...  \n",
      "..           ...                                                ...  \n",
      "293          295  [The, pardons, ,, announced, Monday, ,, includ...  \n",
      "294          296  [They, were, for, people, convicted, of, such,...  \n",
      "295          297  [Including, Monday, 's, actions, ,, Mr., Bush,...  \n",
      "296          298  [Pardons, are, one, of, the, president, 's, ab...  \n",
      "297          299  [White, House, officials, say, he, made, the, ...  \n",
      "\n",
      "[298 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_triplets(df): #generate a new DataFrame of triplets, anchor, positive, negative example.\n",
    "    triplets = [] #create an empty list to store the triplet dictionaries\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index + 2 <= df.index.max():\n",
    "            positive_index = index + 1\n",
    "            negative_index = index + 2\n",
    "\n",
    "            triplets.append({\n",
    "                'anchor_id': row['id'],\n",
    "                'anchor_tokens': row['tokens'],\n",
    "                'positive_id': df.at[positive_index, 'id'],\n",
    "                'positive_tokens': df.at[positive_index, 'tokens'],\n",
    "                'negative_id': df.at[negative_index, 'id'],\n",
    "                'negative_tokens': df.at[negative_index, 'tokens'],\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(triplets)\n",
    "\n",
    "triplets_df = generate_triplets(new_df)\n",
    "\n",
    "if not triplets_df.empty:\n",
    "    print(triplets_df)\n",
    "else:\n",
    "    print(\"No suitable triplets found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor_id', 'anchor_tokens', 'positive_id', 'positive_tokens', 'negative_id', 'negative_tokens'],\n",
       "    num_rows: 298\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_dataset = Dataset.from_pandas(triplets_df)\n",
    "triplet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "#prepares batches for processing with a ML model that uses triplet loss\n",
    "def collate_fn(batch):\n",
    "    anchor_texts = [' '.join(triplet['anchor_tokens']) for triplet in batch]\n",
    "    positive_texts = [' '.join(triplet['positive_tokens']) for triplet in batch]\n",
    "    negative_texts = [' '.join(triplet['negative_tokens']) for triplet in batch]\n",
    "    \n",
    "    # Tokenize and encode texts in the batch\n",
    "    anchor_encodings = tokenizer(anchor_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    positive_encodings = tokenizer(positive_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    negative_encodings = tokenizer(negative_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Separate and structure the encodings as needed\n",
    "    anchor_ids, anchor_attn_masks = anchor_encodings['input_ids'], anchor_encodings['attention_mask']\n",
    "    positive_ids, positive_attn_masks = positive_encodings['input_ids'], positive_encodings['attention_mask']\n",
    "    negative_ids, negative_attn_masks = negative_encodings['input_ids'], negative_encodings['attention_mask']\n",
    "\n",
    "    return (anchor_ids, anchor_attn_masks), (positive_ids, positive_attn_masks), (negative_ids, negative_attn_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The primary purpose of a DataLoader is to automate the process of data sampling, batching, and (optionally) applying transformations to the data.\n",
    "triplet_dataset_train, triplet_dataset_eval = train_test_split(triplets_df, test_size=0.2, random_state=42) \n",
    "triplet_dataset_train = Dataset.from_pandas(triplet_dataset_train)\n",
    "triplet_dataset_eval = Dataset.from_pandas(triplet_dataset_eval)\n",
    "train_dataloader = DataLoader(triplet_dataset_train, batch_size=32, shuffle=True, collate_fn=collate_fn) \n",
    "eval_dataloader = DataLoader(triplet_dataset_eval, batch_size=32, collate_fn=collate_fn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(torch.nn.Module): #encode sequences using a BiLSTM network.\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, bidirectional=True, batch_first=True)\n",
    "    #Defines the forward pass of the encoder\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if attention_mask is not None:\n",
    "            sequence_lengths = attention_mask.sum(dim=1)\n",
    "\n",
    "            # Sort batches by sequence length\n",
    "            sequence_lengths, perm_idx = sequence_lengths.sort(0, descending=True)\n",
    "            input_ids = input_ids[perm_idx]\n",
    "\n",
    "        embedded = self.embedding(input_ids)\n",
    "\n",
    "        # If using sequence lengths, pack the sequence\n",
    "        if attention_mask is not None:\n",
    "            packed_embedded = rnn_utils.pack_padded_sequence(embedded, lengths=sequence_lengths.cpu(), batch_first=True)\n",
    "            packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "            output, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        else:\n",
    "            output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "\n",
    "        # If you reordered the batch by sequence length, reorder back to its original order\n",
    "        if attention_mask is not None:\n",
    "            _, unperm_idx = perm_idx.sort(0)\n",
    "            hidden = hidden[unperm_idx]\n",
    "        #concatenated hidden state for each sequence in the batch, representing the encoded information of each sequence\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Triplet loss function, commonly used in tasks that involve learning from relative comparisons between an anchor sample, a positive sample (similar to the anchor), and a negative sample (dissimilar from the anchor).\n",
    "- The purpose of this loss function is to ensure that, in the learned representation space, the anchor is closer to the positive sample than to the negative sample by at least a margin.\n",
    "- The constructor of the TripletLoss class takes a single parameter, margin, which defaults to 1.0. This margin is a hyperparameter that defines the smallest distance between the positive and negative pairs' distances we want to achieve.\n",
    "The margin is stored as an instance variable to be used in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, loss_fn, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for (anchor_ids, anchor_masks), (positive_ids, positive_masks), (negative_ids, negative_masks) in data_loader:\n",
    "            optimizer.zero_grad() #to prevent accumulation of gradients from multiple backward passes.\n",
    "\n",
    "            # Call the model with only token IDs, ignore the attention masks\n",
    "            anchor_embeddings = model(anchor_ids)\n",
    "            positive_embeddings = model(positive_ids)\n",
    "            negative_embeddings = model(negative_ids)\n",
    "\n",
    "            # Compute the loss using the embeddings\n",
    "            loss = loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "            loss.backward() #computes the gradients of the loss with respect to model parameters.\n",
    "            optimizer.step() #updates the model parameters based on the gradients\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 300  \n",
    "hidden_dim = 100\n",
    "\n",
    "model = BiLSTMEncoder(embedding_dim, hidden_dim, vocab_size)\n",
    "loss_fn = TripletLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3259323835372925\n",
      "Epoch 2, Loss: 0.6622036695480347\n",
      "Epoch 3, Loss: 0.5667781829833984\n",
      "Epoch 4, Loss: 0.25489354133605957\n",
      "Epoch 5, Loss: 0.20623275637626648\n"
     ]
    }
   ],
   "source": [
    "data_loader = train_dataloader\n",
    "num_epochs = 5\n",
    "triplet_dataset = Dataset.from_pandas(triplets_df)\n",
    "train(model, data_loader, loss_fn, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next Step\n",
    "    - Perform cross-validation and hyperparameter tuning for finding the optimal combination of parameters(embedding_dim, hidden_dim) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
