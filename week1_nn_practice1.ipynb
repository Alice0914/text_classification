{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1: Neural Network with PyTorch Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful.\n"
     ]
    }
   ],
   "source": [
    "# For more details of the dataset: https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "filename = \"pima-indians-diabetes.csv\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(filename, 'wb') as file: \n",
    "        file.write(response.content) \n",
    "    print(\"Download successful.\")\n",
    "else:\n",
    "    print(\"Download failed with status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numerical value to a tensor\n",
    "#PyTorch usually operates in a 32-bit floating point while NumPy, by default, uses a 64-bit floating point.\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **Sequential Model Definition**:\n",
    "- A `Sequential` model in PyTorch is created by listing out layers in sequence. \n",
    "- The example provided constructs a model for data with 8 input features using fully connected or dense layers (`nn.Linear`), specifying the input and output dimensions for each layer.\n",
    "- The model includes two hidden layers with 12 and 8 neurons, respectively, each followed by a ReLU activation function to introduce non-linearity.\n",
    "- The output layer consists of a single neuron with a sigmoid activation function, making the model's output suitable for binary classification tasks, mapping the output to a probability between 0 and 1.\n",
    "\n",
    "##### 2. **Class-Based Model Definition**:\n",
    "- The alternative approach to defining a PyTorch model involves creating a Python class that inherits from `nn.Module`.\n",
    "- This class-based model explicitly defines each layer and the activation function as class attributes in the constructor (`__init__` method). It also requires defining a `forward()` method, detailing how the input tensor is processed through the layers to produce an output tensor.\n",
    "- The same structure is used as in the Sequential model, with the layers and activations defined as attributes of the class and the data flow specified in the `forward` method.\n",
    "\n",
    "Both methods allow for the flexibility to experiment with different architectures and layer configurations to optimize model performance. The choice between using a Sequential model or a class-based model typically depends on the complexity of the model and personal preference, with the class-based approach offering more control over the forward pass and the ability to include custom operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#nn.Sequential automatically handles the forward pass in the order the layers are added.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class_Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_classifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class nn_classifier(nn.Module): #specify how data flows through these layers in the forward method.\n",
    "    def __init__(self):\n",
    "        super().__init__() # Call the parent's __init__ first!\n",
    "        self.hidden1 = nn.Linear(8,12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12,8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8,1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return X\n",
    "\n",
    "model = nn_classifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a neural network model, you must define a **loss function** and an **optimizer**. \n",
    "- The **loss function** measures how close the model's predictions are to the true target values. \n",
    "For binary classification tasks, **binary cross-entropy** is a suitable loss function. \n",
    "(Mean Squared Error for regression, Cross-Entropy Loss for classification)\n",
    "- An **optimizer**, such as **Adam**, is used to update the model's weights based on the loss function's feedback to improve its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss() #binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network model usually takes in **epochs** and **batches**.\n",
    "- Each epoch consists of multiple iterations over all the batches, where gradient descent is applied to refine the model's weights. \n",
    "- The process is iteratively repeated across epochs until the model's performance is satisfactory, aiming for a balance between computational efficiency and training duration. \n",
    "- The choice of batch size and the number of epochs typically involves experimentation. \n",
    "- As training progresses, the model's error decreases and eventually stabilizes, indicating convergence.\n",
    "- A training loop is commonly implemented using two nested for-loops, one iterating over epochs and the other over batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimizer.zero_grad()**\n",
    "- Gradients are small changes that show how much the error would change if the model's weights were slightly altered.\n",
    "- In PyTorch, gradients accumulate by default (i.e., they are summed up) to support dynamic computations and recurrent neural networks. \n",
    "- optimizer.zero_grad() sets all the gradients to zero for all model parameters at the beginning of each iteration\n",
    "- Without this call, gradients would accumulate across batches, leading to incorrect updates.\n",
    "\n",
    "**loss.backward()**\n",
    "- It initiates the backpropagation algorithm\n",
    "- It does not compute the loss itself; rather, it calculates the gradients of the loss with respect to the model's parameters during backpropagation.\n",
    "- This means that when the loss.backward() function is executed, it doesn't actually calculate the value of the loss (error). Instead, **based on the already computed value of the loss function, it calculates how this loss affects each parameter of the model, i.e., it computes the gradient (rate of change) of the loss with respect to each parameter. Backpropagation plays a crucial role in using these gradients to update the model's parameters.\n",
    "\n",
    "**optimizer.step()**\n",
    "- After the gradients are calculated and stored, you need to update the model parameters in the direction that minimizes the loss.\n",
    "- The optimizer, which was previously defined (e.g., using torch.optim.Adam), knows how to update each parameter given its current gradient stored in .grad. \n",
    "- Adam adjust the learning rates of each parameter dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishede epoch 0, latest loss 0.5624324679374695\n",
      "Finishede epoch 1, latest loss 0.5624324679374695\n",
      "Finishede epoch 2, latest loss 0.5624324679374695\n",
      "Finishede epoch 3, latest loss 0.5624324679374695\n",
      "Finishede epoch 4, latest loss 0.5624324679374695\n",
      "Finishede epoch 5, latest loss 0.5624324679374695\n",
      "Finishede epoch 6, latest loss 0.5624324679374695\n",
      "Finishede epoch 7, latest loss 0.5624324679374695\n",
      "Finishede epoch 8, latest loss 0.5624324679374695\n",
      "Finishede epoch 9, latest loss 0.5624324679374695\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i: i+batch_size]\n",
    "        y_pred = model(Xbatch) # Forward pass\n",
    "        ybatch = y[i: i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() # Initiates the backpropagation algorithm\n",
    "        optimizer.step() # Update model parameters\n",
    "    print(f'Finishede epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.6510416865348816\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy (no_grad is optional)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    " \n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "Finished epoch 0, latest loss 0.5492443442344666\n",
      "Finished epoch 1, latest loss 0.6172183156013489\n",
      "Finished epoch 2, latest loss 0.5994867086410522\n",
      "Finished epoch 3, latest loss 0.5781427025794983\n",
      "Finished epoch 4, latest loss 0.560743510723114\n",
      "Finished epoch 5, latest loss 0.5461434721946716\n",
      "Finished epoch 6, latest loss 0.5438551306724548\n",
      "Finished epoch 7, latest loss 0.5339827537536621\n",
      "Finished epoch 8, latest loss 0.5248621106147766\n",
      "Finished epoch 9, latest loss 0.51628178358078\n",
      "Finished epoch 10, latest loss 0.5057646036148071\n",
      "Finished epoch 11, latest loss 0.4977971017360687\n",
      "Finished epoch 12, latest loss 0.4928841292858124\n",
      "Finished epoch 13, latest loss 0.48636361956596375\n",
      "Finished epoch 14, latest loss 0.4819941520690918\n",
      "Finished epoch 15, latest loss 0.4782876968383789\n",
      "Finished epoch 16, latest loss 0.4734359383583069\n",
      "Finished epoch 17, latest loss 0.4666684567928314\n",
      "Finished epoch 18, latest loss 0.468202143907547\n",
      "Finished epoch 19, latest loss 0.4625776410102844\n",
      "Finished epoch 20, latest loss 0.45966705679893494\n",
      "Finished epoch 21, latest loss 0.45934396982192993\n",
      "Finished epoch 22, latest loss 0.4514780640602112\n",
      "Finished epoch 23, latest loss 0.4511311948299408\n",
      "Finished epoch 24, latest loss 0.4532633125782013\n",
      "Finished epoch 25, latest loss 0.44885286688804626\n",
      "Finished epoch 26, latest loss 0.4476953446865082\n",
      "Finished epoch 27, latest loss 0.44487515091896057\n",
      "Finished epoch 28, latest loss 0.44225364923477173\n",
      "Finished epoch 29, latest loss 0.44594112038612366\n",
      "Finished epoch 30, latest loss 0.4443226158618927\n",
      "Finished epoch 31, latest loss 0.4431365728378296\n",
      "Finished epoch 32, latest loss 0.4282817840576172\n",
      "Finished epoch 33, latest loss 0.4276249408721924\n",
      "Finished epoch 34, latest loss 0.43907174468040466\n",
      "Finished epoch 35, latest loss 0.43542543053627014\n",
      "Finished epoch 36, latest loss 0.422063946723938\n",
      "Finished epoch 37, latest loss 0.4335760772228241\n",
      "Finished epoch 38, latest loss 0.4191230535507202\n",
      "Finished epoch 39, latest loss 0.43056604266166687\n",
      "Finished epoch 40, latest loss 0.4171927273273468\n",
      "Finished epoch 41, latest loss 0.4282129406929016\n",
      "Finished epoch 42, latest loss 0.4206998944282532\n",
      "Finished epoch 43, latest loss 0.4201822876930237\n",
      "Finished epoch 44, latest loss 0.43268969655036926\n",
      "Finished epoch 45, latest loss 0.41781413555145264\n",
      "Finished epoch 46, latest loss 0.4176638722419739\n",
      "Finished epoch 47, latest loss 0.42903825640678406\n",
      "Finished epoch 48, latest loss 0.41574740409851074\n",
      "Finished epoch 49, latest loss 0.4150451123714447\n",
      "Finished epoch 50, latest loss 0.4261036813259125\n",
      "Finished epoch 51, latest loss 0.41762956976890564\n",
      "Finished epoch 52, latest loss 0.42651793360710144\n",
      "Finished epoch 53, latest loss 0.4141579270362854\n",
      "Finished epoch 54, latest loss 0.4170885980129242\n",
      "Finished epoch 55, latest loss 0.43250608444213867\n",
      "Finished epoch 56, latest loss 0.42185652256011963\n",
      "Finished epoch 57, latest loss 0.4250180125236511\n",
      "Finished epoch 58, latest loss 0.4217321574687958\n",
      "Finished epoch 59, latest loss 0.42016226053237915\n",
      "Finished epoch 60, latest loss 0.4218255579471588\n",
      "Finished epoch 61, latest loss 0.4207504391670227\n",
      "Finished epoch 62, latest loss 0.4174274206161499\n",
      "Finished epoch 63, latest loss 0.4185316860675812\n",
      "Finished epoch 64, latest loss 0.4180477559566498\n",
      "Finished epoch 65, latest loss 0.41633662581443787\n",
      "Finished epoch 66, latest loss 0.4159587025642395\n",
      "Finished epoch 67, latest loss 0.4159510135650635\n",
      "Finished epoch 68, latest loss 0.41526708006858826\n",
      "Finished epoch 69, latest loss 0.41622716188430786\n",
      "Finished epoch 70, latest loss 0.4159063398838043\n",
      "Finished epoch 71, latest loss 0.41432082653045654\n",
      "Finished epoch 72, latest loss 0.41031697392463684\n",
      "Finished epoch 73, latest loss 0.41116708517074585\n",
      "Finished epoch 74, latest loss 0.41260001063346863\n",
      "Finished epoch 75, latest loss 0.410446435213089\n",
      "Finished epoch 76, latest loss 0.4070552587509155\n",
      "Finished epoch 77, latest loss 0.408409059047699\n",
      "Finished epoch 78, latest loss 0.40926283597946167\n",
      "Finished epoch 79, latest loss 0.41228732466697693\n",
      "Finished epoch 80, latest loss 0.4084075689315796\n",
      "Finished epoch 81, latest loss 0.4050735831260681\n",
      "Finished epoch 82, latest loss 0.40641143918037415\n",
      "Finished epoch 83, latest loss 0.40567493438720703\n",
      "Finished epoch 84, latest loss 0.40334558486938477\n",
      "Finished epoch 85, latest loss 0.4044915735721588\n",
      "Finished epoch 86, latest loss 0.4035874903202057\n",
      "Finished epoch 87, latest loss 0.40069884061813354\n",
      "Finished epoch 88, latest loss 0.4015125632286072\n",
      "Finished epoch 89, latest loss 0.4045957326889038\n",
      "Finished epoch 90, latest loss 0.40026140213012695\n",
      "Finished epoch 91, latest loss 0.3981735110282898\n",
      "Finished epoch 92, latest loss 0.4233992099761963\n",
      "Finished epoch 93, latest loss 0.39961719512939453\n",
      "Finished epoch 94, latest loss 0.39305418729782104\n",
      "Finished epoch 95, latest loss 0.39460355043411255\n",
      "Finished epoch 96, latest loss 0.39451122283935547\n",
      "Finished epoch 97, latest loss 0.3931552469730377\n",
      "Finished epoch 98, latest loss 0.3905445337295532\n",
      "Finished epoch 99, latest loss 0.39183127880096436\n",
      "Accuracy 0.7591145634651184\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    " \n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    " \n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    " \n",
    "# define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print(model)\n",
    " \n",
    "# train the model\n",
    "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    " \n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')\n",
    " \n",
    "# compute accuracy (no_grad is optional)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add L1 and L2 Regularization to prevent overfitting\n",
    "- L1 regularization: l1_loss is calculated by summing the absolute values of all the model parameters\n",
    "- L2 regularization: Used the weight_decay parameter in the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "Finished epoch 0, latest total loss 0.6232933402061462\n",
      "Finished epoch 1, latest total loss 0.6222648024559021\n",
      "Finished epoch 2, latest total loss 0.6221972703933716\n",
      "Finished epoch 3, latest total loss 0.6166897416114807\n",
      "Finished epoch 4, latest total loss 0.6068466305732727\n",
      "Finished epoch 5, latest total loss 0.5833206176757812\n",
      "Finished epoch 6, latest total loss 0.5786837339401245\n",
      "Finished epoch 7, latest total loss 0.5704125761985779\n",
      "Finished epoch 8, latest total loss 0.5675392746925354\n",
      "Finished epoch 9, latest total loss 0.5628054738044739\n",
      "Finished epoch 10, latest total loss 0.5587262511253357\n",
      "Finished epoch 11, latest total loss 0.5553116798400879\n",
      "Finished epoch 12, latest total loss 0.5542548298835754\n",
      "Finished epoch 13, latest total loss 0.548955500125885\n",
      "Finished epoch 14, latest total loss 0.5480598211288452\n",
      "Finished epoch 15, latest total loss 0.5415701270103455\n",
      "Finished epoch 16, latest total loss 0.5402645468711853\n",
      "Finished epoch 17, latest total loss 0.5367296934127808\n",
      "Finished epoch 18, latest total loss 0.5415001511573792\n",
      "Finished epoch 19, latest total loss 0.5338193774223328\n",
      "Finished epoch 20, latest total loss 0.5340130925178528\n",
      "Finished epoch 21, latest total loss 0.5320151448249817\n",
      "Finished epoch 22, latest total loss 0.5354219675064087\n",
      "Finished epoch 23, latest total loss 0.5191254019737244\n",
      "Finished epoch 24, latest total loss 0.5284003019332886\n",
      "Finished epoch 25, latest total loss 0.5114833116531372\n",
      "Finished epoch 26, latest total loss 0.5071970224380493\n",
      "Finished epoch 27, latest total loss 0.5184836983680725\n",
      "Finished epoch 28, latest total loss 0.5001201629638672\n",
      "Finished epoch 29, latest total loss 0.4974600672721863\n",
      "Finished epoch 30, latest total loss 0.49061718583106995\n",
      "Finished epoch 31, latest total loss 0.4974421560764313\n",
      "Finished epoch 32, latest total loss 0.4888438284397125\n",
      "Finished epoch 33, latest total loss 0.4893483519554138\n",
      "Finished epoch 34, latest total loss 0.4875677824020386\n",
      "Finished epoch 35, latest total loss 0.4861283302307129\n",
      "Finished epoch 36, latest total loss 0.48037779331207275\n",
      "Finished epoch 37, latest total loss 0.48169898986816406\n",
      "Finished epoch 38, latest total loss 0.4768988788127899\n",
      "Finished epoch 39, latest total loss 0.4753745198249817\n",
      "Finished epoch 40, latest total loss 0.4701336920261383\n",
      "Finished epoch 41, latest total loss 0.46942177414894104\n",
      "Finished epoch 42, latest total loss 0.46520379185676575\n",
      "Finished epoch 43, latest total loss 0.456168532371521\n",
      "Finished epoch 44, latest total loss 0.45643842220306396\n",
      "Finished epoch 45, latest total loss 0.45014598965644836\n",
      "Finished epoch 46, latest total loss 0.44810789823532104\n",
      "Finished epoch 47, latest total loss 0.44750282168388367\n",
      "Finished epoch 48, latest total loss 0.44868195056915283\n",
      "Finished epoch 49, latest total loss 0.4375469386577606\n",
      "Finished epoch 50, latest total loss 0.44012778997421265\n",
      "Finished epoch 51, latest total loss 0.4402164816856384\n",
      "Finished epoch 52, latest total loss 0.4403909742832184\n",
      "Finished epoch 53, latest total loss 0.4319635331630707\n",
      "Finished epoch 54, latest total loss 0.42684924602508545\n",
      "Finished epoch 55, latest total loss 0.43031561374664307\n",
      "Finished epoch 56, latest total loss 0.41782301664352417\n",
      "Finished epoch 57, latest total loss 0.41934555768966675\n",
      "Finished epoch 58, latest total loss 0.41062507033348083\n",
      "Finished epoch 59, latest total loss 0.4058998227119446\n",
      "Finished epoch 60, latest total loss 0.4086449146270752\n",
      "Finished epoch 61, latest total loss 0.3940775394439697\n",
      "Finished epoch 62, latest total loss 0.40516483783721924\n",
      "Finished epoch 63, latest total loss 0.3983372747898102\n",
      "Finished epoch 64, latest total loss 0.3959871828556061\n",
      "Finished epoch 65, latest total loss 0.39094048738479614\n",
      "Finished epoch 66, latest total loss 0.384313702583313\n",
      "Finished epoch 67, latest total loss 0.3823533356189728\n",
      "Finished epoch 68, latest total loss 0.3779608905315399\n",
      "Finished epoch 69, latest total loss 0.372601717710495\n",
      "Finished epoch 70, latest total loss 0.37547141313552856\n",
      "Finished epoch 71, latest total loss 0.36635059118270874\n",
      "Finished epoch 72, latest total loss 0.36416468024253845\n",
      "Finished epoch 73, latest total loss 0.3617502748966217\n",
      "Finished epoch 74, latest total loss 0.35962310433387756\n",
      "Finished epoch 75, latest total loss 0.36139363050460815\n",
      "Finished epoch 76, latest total loss 0.35507166385650635\n",
      "Finished epoch 77, latest total loss 0.3549424707889557\n",
      "Finished epoch 78, latest total loss 0.3534491956233978\n",
      "Finished epoch 79, latest total loss 0.35237470269203186\n",
      "Finished epoch 80, latest total loss 0.3490118086338043\n",
      "Finished epoch 81, latest total loss 0.34735023975372314\n",
      "Finished epoch 82, latest total loss 0.3426983058452606\n",
      "Finished epoch 83, latest total loss 0.3529130816459656\n",
      "Finished epoch 84, latest total loss 0.3297460973262787\n",
      "Finished epoch 85, latest total loss 0.3468257188796997\n",
      "Finished epoch 86, latest total loss 0.3411889970302582\n",
      "Finished epoch 87, latest total loss 0.3294373154640198\n",
      "Finished epoch 88, latest total loss 0.3274419605731964\n",
      "Finished epoch 89, latest total loss 0.3426719605922699\n",
      "Finished epoch 90, latest total loss 0.3342670798301697\n",
      "Finished epoch 91, latest total loss 0.3198864459991455\n",
      "Finished epoch 92, latest total loss 0.33757591247558594\n",
      "Finished epoch 93, latest total loss 0.3264780640602112\n",
      "Finished epoch 94, latest total loss 0.3146069347858429\n",
      "Finished epoch 95, latest total loss 0.3321629464626312\n",
      "Finished epoch 96, latest total loss 0.3112539052963257\n",
      "Finished epoch 97, latest total loss 0.3339465856552124\n",
      "Finished epoch 98, latest total loss 0.308247834444046\n",
      "Finished epoch 99, latest total loss 0.3033885657787323\n",
      "Accuracy: 0.76171875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:, 0:8]\n",
    "y = dataset[:, 8]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Define loss function (binary cross entropy) and optimizer with L2 regularization\n",
    "loss_fn = nn.BCELoss()\n",
    "l2_lambda = 0.001  # L2 regularization weight\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i + batch_size]\n",
    "        ybatch = y[i:i + batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "\n",
    "        # L1 regularization: manually add L1 loss for all parameters\n",
    "        l1_lambda = 0.0005  # L1 regularization weight\n",
    "        l1_loss = sum(p.abs().sum() for p in model.parameters())\n",
    "        total_loss = loss + l1_lambda * l1_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Finished epoch {epoch}, latest total loss {total_loss.item()}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression for a classification model\n",
    "\n",
    "- Logistic regression is essentially a single-layer neural network without hidden layers and uses a sigmoid activation function for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Model with PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (1): Sigmoid()\n",
      ")\n",
      "Finished epoch 0, latest loss 75.0\n",
      "Finished epoch 1, latest loss 75.0\n",
      "Finished epoch 2, latest loss 75.0\n",
      "Finished epoch 3, latest loss 75.0\n",
      "Finished epoch 4, latest loss 75.0\n",
      "Finished epoch 5, latest loss 3.3385066986083984\n",
      "Finished epoch 6, latest loss 1.7819201946258545\n",
      "Finished epoch 7, latest loss 1.4942830801010132\n",
      "Finished epoch 8, latest loss 1.2700145244598389\n",
      "Finished epoch 9, latest loss 1.1119365692138672\n",
      "Finished epoch 10, latest loss 0.9939742684364319\n",
      "Finished epoch 11, latest loss 0.907599925994873\n",
      "Finished epoch 12, latest loss 0.8482560515403748\n",
      "Finished epoch 13, latest loss 0.8095002174377441\n",
      "Finished epoch 14, latest loss 0.7833399772644043\n",
      "Finished epoch 15, latest loss 0.7631639242172241\n",
      "Finished epoch 16, latest loss 0.7454894185066223\n",
      "Finished epoch 17, latest loss 0.7291943430900574\n",
      "Finished epoch 18, latest loss 0.7140536308288574\n",
      "Finished epoch 19, latest loss 0.6999861598014832\n",
      "Finished epoch 20, latest loss 0.6868898868560791\n",
      "Finished epoch 21, latest loss 0.6746564507484436\n",
      "Finished epoch 22, latest loss 0.6632078289985657\n",
      "Finished epoch 23, latest loss 0.6525034308433533\n",
      "Finished epoch 24, latest loss 0.6425307393074036\n",
      "Finished epoch 25, latest loss 0.6332889199256897\n",
      "Finished epoch 26, latest loss 0.6247751116752625\n",
      "Finished epoch 27, latest loss 0.6169772744178772\n",
      "Finished epoch 28, latest loss 0.6098722815513611\n",
      "Finished epoch 29, latest loss 0.6034262776374817\n",
      "Finished epoch 30, latest loss 0.5975978970527649\n",
      "Finished epoch 31, latest loss 0.5923404693603516\n",
      "Finished epoch 32, latest loss 0.5876049995422363\n",
      "Finished epoch 33, latest loss 0.5833423137664795\n",
      "Finished epoch 34, latest loss 0.5795044898986816\n",
      "Finished epoch 35, latest loss 0.5760461688041687\n",
      "Finished epoch 36, latest loss 0.5729255080223083\n",
      "Finished epoch 37, latest loss 0.5701038837432861\n",
      "Finished epoch 38, latest loss 0.5675459504127502\n",
      "Finished epoch 39, latest loss 0.5652207732200623\n",
      "Finished epoch 40, latest loss 0.5630998015403748\n",
      "Finished epoch 41, latest loss 0.5611583590507507\n",
      "Finished epoch 42, latest loss 0.5593745112419128\n",
      "Finished epoch 43, latest loss 0.5577287673950195\n",
      "Finished epoch 44, latest loss 0.5562042593955994\n",
      "Finished epoch 45, latest loss 0.5547856688499451\n",
      "Finished epoch 46, latest loss 0.5534602999687195\n",
      "Finished epoch 47, latest loss 0.5522164106369019\n",
      "Finished epoch 48, latest loss 0.5510440468788147\n",
      "Finished epoch 49, latest loss 0.5499348640441895\n",
      "Finished epoch 50, latest loss 0.5488807559013367\n",
      "Finished epoch 51, latest loss 0.5478752255439758\n",
      "Finished epoch 52, latest loss 0.5469124913215637\n",
      "Finished epoch 53, latest loss 0.5459874272346497\n",
      "Finished epoch 54, latest loss 0.5450958609580994\n",
      "Finished epoch 55, latest loss 0.5442337393760681\n",
      "Finished epoch 56, latest loss 0.5433977842330933\n",
      "Finished epoch 57, latest loss 0.542585015296936\n",
      "Finished epoch 58, latest loss 0.5417929291725159\n",
      "Finished epoch 59, latest loss 0.5410191416740417\n",
      "Finished epoch 60, latest loss 0.5402620434761047\n",
      "Finished epoch 61, latest loss 0.5395194292068481\n",
      "Finished epoch 62, latest loss 0.5387902855873108\n",
      "Finished epoch 63, latest loss 0.5380731821060181\n",
      "Finished epoch 64, latest loss 0.5373668074607849\n",
      "Finished epoch 65, latest loss 0.5366701483726501\n",
      "Finished epoch 66, latest loss 0.535982608795166\n",
      "Finished epoch 67, latest loss 0.5353031754493713\n",
      "Finished epoch 68, latest loss 0.5346312522888184\n",
      "Finished epoch 69, latest loss 0.5339664220809937\n",
      "Finished epoch 70, latest loss 0.5333079099655151\n",
      "Finished epoch 71, latest loss 0.5326555371284485\n",
      "Finished epoch 72, latest loss 0.5320086479187012\n",
      "Finished epoch 73, latest loss 0.5313670635223389\n",
      "Finished epoch 74, latest loss 0.5307303667068481\n",
      "Finished epoch 75, latest loss 0.5300982594490051\n",
      "Finished epoch 76, latest loss 0.5294705629348755\n",
      "Finished epoch 77, latest loss 0.5288471579551697\n",
      "Finished epoch 78, latest loss 0.5282278060913086\n",
      "Finished epoch 79, latest loss 0.5276122689247131\n",
      "Finished epoch 80, latest loss 0.5270004868507385\n",
      "Finished epoch 81, latest loss 0.5263922214508057\n",
      "Finished epoch 82, latest loss 0.5257874727249146\n",
      "Finished epoch 83, latest loss 0.5251860618591309\n",
      "Finished epoch 84, latest loss 0.5245880484580994\n",
      "Finished epoch 85, latest loss 0.523993194103241\n",
      "Finished epoch 86, latest loss 0.5234014391899109\n",
      "Finished epoch 87, latest loss 0.5228127241134644\n",
      "Finished epoch 88, latest loss 0.5222270488739014\n",
      "Finished epoch 89, latest loss 0.5216443538665771\n",
      "Finished epoch 90, latest loss 0.5210645198822021\n",
      "Finished epoch 91, latest loss 0.5204875469207764\n",
      "Finished epoch 92, latest loss 0.5199134945869446\n",
      "Finished epoch 93, latest loss 0.5193422436714172\n",
      "Finished epoch 94, latest loss 0.5187734961509705\n",
      "Finished epoch 95, latest loss 0.5182077884674072\n",
      "Finished epoch 96, latest loss 0.5176445841789246\n",
      "Finished epoch 97, latest loss 0.5170841813087463\n",
      "Finished epoch 98, latest loss 0.5165265202522278\n",
      "Finished epoch 99, latest loss 0.515971302986145\n",
      "Accuracy: 0.73828125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:, 0:8]\n",
    "y = dataset[:, 8]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Define the logistic regression model\n",
    "log_reg_model = nn.Sequential(\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print(log_reg_model)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "l2_lambda = 0.001\n",
    "optimizer = optim.Adam(log_reg_model.parameters(), lr=0.001, weight_decay=l2_lambda)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i + batch_size]\n",
    "        ybatch = y[i:i + batch_size]\n",
    "        y_pred = log_reg_model(Xbatch)\n",
    "\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss.item()}')\n",
    "\n",
    "# Compute accuracy\n",
    "with torch.no_grad():\n",
    "    y_pred = log_reg_model(X)\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Model with Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7736156351791531\n",
      "Testing Accuracy: 0.7402597402597403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:, 0:8]\n",
    "y = dataset[:, 8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(C= 0.0018329807108324356, penalty='l2', solver='newton-cg', max_iter=5000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using GridSearchCV from Scikit-learn on the logistic regression mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 345 candidates, totalling 1725 fits\n",
      "Best Parameters: {'C': 0.0018329807108324356, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Best Cross-validation Score: 0.7687724910035986\n",
      "Testing Accuracy with Best Parameters: 0.7402597402597403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "25 fits failed out of a total of 1725.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.65309876 0.65309876 0.65970945 0.65970945 0.65309876 0.65309876\n",
      " 0.66294815 0.66294815 0.65309876 0.65309876 0.66945222 0.66945222\n",
      " 0.64663468 0.64663468 0.67109156 0.67109156 0.66622684 0.66622684\n",
      " 0.66783953 0.66783953 0.66621351 0.66621351 0.66947888 0.66783953\n",
      " 0.67271758 0.6743436  0.68740504 0.66457417 0.71833933 0.66621351\n",
      " 0.70206584 0.6694922  0.76385446 0.66621351 0.718326   0.6694922\n",
      " 0.76713315 0.66457417 0.7475943  0.6694922  0.76552046 0.66457417\n",
      " 0.76061575 0.6694922  0.76714647 0.6694922  0.76874583 0.6694922\n",
      " 0.76550713 0.6694922  0.76552046 0.6694922  0.76550713 0.6694922\n",
      " 0.76552046 0.6694922  0.76550713 0.6694922  0.76388111 0.6694922\n",
      " 0.76550713 0.6694922  0.76550713 0.6694922  0.76550713 0.6694922\n",
      " 0.76550713 0.6694922  0.76550713 0.6694922  0.76550713 0.6694922\n",
      " 0.76550713 0.6694922  0.76550713 0.6694922  0.76550713 0.6694922\n",
      " 0.76550713 0.6694922  0.75404505 0.75404505 0.65970945 0.75405838\n",
      " 0.75405838 0.66294815 0.76224177 0.76224177 0.66945222 0.76877249\n",
      " 0.76877249 0.67109156 0.76877249 0.76877249 0.66783953 0.76875916\n",
      " 0.76875916 0.66783953 0.76714647 0.76714647 0.67275756 0.76388111\n",
      " 0.76388111 0.68254032 0.76714647 0.76714647 0.6809143  0.76714647\n",
      " 0.76550713 0.6809143  0.76713315 0.76713315 0.68254032 0.76713315\n",
      " 0.76713315 0.68254032 0.76550713 0.76550713 0.68254032 0.76550713\n",
      " 0.76550713 0.68254032 0.76550713 0.76550713 0.68254032 0.76550713\n",
      " 0.76550713 0.68254032 0.76550713 0.76550713 0.68254032 0.76550713\n",
      " 0.76550713 0.68254032 0.76550713 0.76550713 0.68254032 0.76550713\n",
      " 0.76550713 0.68254032 0.65970945 0.65309876 0.65309876 0.65309876\n",
      " 0.65309876 0.65309876 0.65309876 0.65309876 0.65309876 0.65309876\n",
      " 0.66294815 0.64990004 0.6368386  0.65309876 0.65309876 0.65309876\n",
      " 0.65309876 0.65309876 0.65309876 0.65309876 0.66945222 0.66133547\n",
      " 0.6645875  0.65152606 0.64013061 0.63521258 0.65309876 0.65309876\n",
      " 0.65309876 0.65309876 0.67109156 0.6678262  0.66132214 0.6645875\n",
      " 0.66296148 0.6645875  0.65805678 0.65479142 0.65152606 0.64663468\n",
      " 0.66783953 0.67271758 0.6743436  0.67110489 0.66296148 0.66294815\n",
      " 0.6645875  0.6645875  0.66296148 0.66622684 0.66783953 0.66621351\n",
      " 0.67109156 0.6743436  0.67596961 0.6743436  0.6743436  0.67598294\n",
      " 0.67109156 0.66621351 0.66457417 0.66621351 0.66783953 0.66621351\n",
      " 0.66621351 0.66621351 0.67273091 0.67598294 0.67759563 0.6743436\n",
      " 0.6694922  0.66457417 0.66621351 0.66621351 0.66621351 0.66621351\n",
      " 0.66621351 0.66783953 0.66783953 0.66621351 0.6694922  0.66294815\n",
      " 0.66457417 0.66457417 0.66457417 0.66457417 0.66621351 0.66621351\n",
      " 0.66621351 0.66621351 0.6694922  0.6694922  0.6694922  0.66294815\n",
      " 0.66294815 0.66294815 0.66294815 0.66457417 0.66457417 0.66457417\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.66621351 0.66457417 0.66294815 0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922  0.6694922  0.6694922\n",
      " 0.6694922  0.6694922  0.6694922  0.6694922         nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:, 0:8]\n",
    "y = dataset[:, 8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "params_grid = [\n",
    "    {'solver': ['liblinear', 'saga'], 'penalty': ['l1', 'l2'], \n",
    "     'C': np.logspace(-4, 4, 20)},\n",
    "    {'solver': ['newton-cg', 'lbfgs', 'sag'], 'penalty': ['l2'], \n",
    "     'C': np.logspace(-4, 4, 20)},\n",
    "    {'solver': ['saga'], 'penalty': ['elasticnet'], 'C': np.logspace(-4, 4, 20), \n",
    "     'l1_ratio': np.linspace(0, 1, 10)},\n",
    "    {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'penalty': ['none']}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params_grid, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-validation Score:\", grid_search.best_score_)\n",
    "\n",
    "y_pred_test = grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Testing Accuracy with Best Parameters: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
