{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before we dive into the three most common subword tokenization algorithms used with Transformer models, take a look at the preprocessing the steps in the tokenization pipeline: \n",
    "- Normalization -> Pre-tokenization -> model -> postprocessor\n",
    "- The tokenizer performs two steps: normalization and pre-tokenization.\n",
    "- 3 most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alice\\Documents\\NLP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization: normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-tokenization: split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training. we can use the pre_tokenize_str() method of the pre_tokenizer attribute of the tokenizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unlike the BERT tokenizer, this tokenizer does not ignore the double space.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (_), \n",
    "#but the T5 tokenizer only splits on whitespace, not punctuation.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
     ]
    }
   ],
   "source": [
    "#three main subword tokenization algorithms: \n",
    "#BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others)\n",
    "#https://huggingface.co/learn/nlp-course/en/chapter6/4?fw=pt\n",
    "\n",
    "#BPE\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ġ', 'i'): 2\n",
      "('Ġ', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ġ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
    "vocab.append(\"Ġt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
    "print(splits[\"Ġtrained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **summary**\n",
    "- Consider a simple corpus: \"hug\", \"pug\", \"pun\", \"bun\", \"hugs\". The base vocabulary starts as [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]. The pair \"u\", \"g\" appears most frequently across words, so it is merged into a new token \"ug\". The corpus then updates words containing \"ug\", and the vocabulary now includes \"ug\". This process repeats, gradually creating larger tokens, such as merging \"u\", \"n\" into \"un\", and so on, building a vocabulary that represents the text efficiently.\n",
    "- Key Benefits for NLP balance between character-level and full-word tokenizations adapt to new words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **WordPiece**\n",
    "- WordPiece is a tokenization algorithm developed by Google for pretraining BERT and has been utilized in various Transformer models like DistilBERT, MobileBERT. Unlike BPE which chooses the most frequent pair for merging, WordPiece calculates a score for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.1111111111111111\n",
      "('##h', '##i'): 0.02564102564102564\n",
      "('##i', '##s'): 0.029585798816568046\n",
      "('Ġ', '##i'): 0.005698005698005698\n",
      "('Ġ', '##t'): 0.018518518518518517\n",
      "('##t', '##h'): 0.023809523809523808\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Unigram tokenization**\n",
    "- Unlike BPE and WordPiece, which start with a small vocabulary and expand it by learning rules, Unigram begins with a large vocabulary and prunes tokens until reaching the desired size.\n",
    "- It evaluates all possible segmentations of a given word and selects the segmentation with the highest probability, based on token frequencies. This model treats all tokens as independent, simplifying the calculation of a word's likelihood.\n",
    "\n",
    "    1. Establishes an initial large vocabulary and calculates probabilities based on token frequencies.\n",
    "    2. For a given word, considers all possible segmentations, calculates the probability of each, and selects the one with the highest probability.\n",
    "    3. Calculates the total loss using the current vocabulary for the entire corpus and identifies less necessary tokens for removal based on the increase in loss they would cause.\n",
    "\n",
    "- In the Unigram tokenization process, the increase in total loss when a specific token is removed from the vocabulary is calculated Tokens that result in a smaller increase are considered \"less important\" for the model and become candidates for removal from the vocabulary. This method progressively reduces the size of the vocabulary, ultimately deriving the most efficient and meaningful set of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generative Adversarial Network (GAN)**\n",
    "https://www.geeksforgeeks.org/generative-adversarial-network-gan/\n",
    "- Generative Adversarial Networks (GANs) are a powerful class of neural networks that are used for an unsupervised learning. \n",
    "- The goal of generative modeling is to autonomously identify patterns in input data, enabling the model to produce new examples that feasibly resemble the original dataset.\n",
    "- GANs are made up of two neural networks, a discriminator and a generator. They use adversarial training to produce artificial data that is identical to actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Architecture of GANs**\n",
    "\n",
    "<div>\n",
    "<img src = \"nlp_images/GAN.png\" width = \"700\">\n",
    "</div>\n",
    "\n",
    "- A Generative Adversarial Network (GAN) is composed of two primary parts, which are the Generator and the Discriminator.\n",
    "- Generator\n",
    "    - The generator model creates realistic data from random noise, continually refining its output to mirror actual data through an adaptive training process. It aims to minimize its loss by producing samples that effectively deceive the discriminator into classifying them as authentic.\n",
    "\n",
    "- Discriminator\n",
    "    - The discriminator serves as a binary classifier, distinguishing between real data and synthetic samples produced by the generator by evaluating their authenticity. It learns to improve its accuracy over time, aiming to maximize its ability to identify generated data as fake and real data as authentic, thereby refining the GAN's output to produce highly realistic synthetic data.\n",
    "    \n",
    "1. **Initialization**: Two neural networks, the Generator (G) and the Discriminator (D), are set up, where G aims to generate data resembling real data, and D evaluates the authenticity of data from both G and actual datasets.\n",
    "\n",
    "2. **Generator’s First Move**: G starts by converting a random noise vector into a data sample (e.g., an image), simulating the process of creating new, realistic data.\n",
    "\n",
    "3. **Discriminator’s Turn**: D assesses both real data from its training set and the synthetic data from G, assigning a probability score to gauge if the data is real (closer to 1) or fake (closer to 0).\n",
    "\n",
    "4. **The Learning Process**: The adversarial dynamic kicks in, rewarding both networks when D accurately classifies real and fake data but also encouraging G to improve its output to better deceive D.\n",
    "\n",
    "5. **Generator’s Improvement**: G is updated positively when it successfully fools D, motivating it to enhance its data generation to be more convincing.\n",
    "\n",
    "6. **Discriminator’s Adaptation**: D is reinforced for correctly identifying fake data, sharpening its ability to discriminate between real and generated samples, which in turn drives the iterative improvement of both models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN**\n",
    "https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Architecture of a traditional RNN**\n",
    "- Recurrent neural networks, also known as RNNs, are a class of neural networks that allow previous outputs to be used as inputs while having hidden states. They are typically as follows:\n",
    "\n",
    "<div>\n",
    "<img src = \"nlp_images/RNN.png\" width = \"900\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Applications of RNN**\n",
    "\n",
    "<div>\n",
    "<img src = \"nlp_images/Applications_RNNs.png\" width = \"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Loss function**\n",
    "- In the case of a recurrent neural network, the loss function *L* of all time steps is defined based on the loss at every time step as follows:\n",
    "    <div>\n",
    "    <img src = \"nlp_images/LossFunction.png\" width = \"200\">\n",
    "    </div>\n",
    "\n",
    "##### **Backpropagation through time**\n",
    "- Backpropagation is done at each point in time. At timestep *T*, the derivative of the loss *L* with respect to weight matrix *W* is expressed as floows:\n",
    "    <div>\n",
    "    <img src = \"nlp_images/Backpropagation.png\" width = \"180\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Commonly used activation functions**\n",
    "- The most common activation functions used in RNN modules are described below:\n",
    "    <div>\n",
    "    <img src = \"nlp_images/ActivationFunctions.png\" width = \"800\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNNs are useful for tasks involving sequential data, like sentence generation and machine translation\n",
    "- However, RNNs struggle with long-term dependencies due to the following two reasons\n",
    "    1. **Vanishing/exploding gradient**: Vanishing(if wt<1) or Exploding(if wt>1) Gradients During training beacuase the errors are multiplied by weights at each time step\n",
    "    2. **Short-Term Memory Bias**: The basic RNN structure tends to be heavily influenced by recent inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient clipping**: It is a technique used to cope with the exploding gradient problem sometimes encountered when performing backpropagation. By capping the maximum value for the gradient, this phenomenon is controlled in practice.\n",
    "    <div>\n",
    "    <img src = \"nlp_images/gradient_clipping.png\" width = \"300\">\n",
    "    </div>\n",
    "\n",
    "- In order to remedy the vanishing gradient problem, specific gates are used in some types of RNNs and usually have a well-defined purpose. They are usually noted **Γ** and are equal to:\n",
    "    <div>\n",
    "    <img src = \"nlp_images/gates.png\" width = \"550\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM (Long Short-Term Memory units)**\n",
    "<div>\n",
    "<img src = \"nlp_images/GRU_LSTM.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **A single LSTM Cell**\n",
    "- LSTMs (long short-term memory) are a type of RNN that can learn long-term dependencies\n",
    "- https://medium.com/analytics-vidhya/lstms-explained-a-complete-technically-accurate-conceptual-guide-with-keras-2a650327e8f2\n",
    "<div>\n",
    "<img src = \"nlp_images/LSTM.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The *hidden state* serves as an encoding of the most recent input, capturing immediate past information relevant for tasks like sentiment analysis or next-word prediction. \n",
    "- The *cell state*, on the other hand, acts as the network's long-term memory, integrating data from all previous time steps through selective filtering by forget and input gates, allowing LSTMs to dynamically decide the importance of historical information in time-series data analysis. Together, these components enable LSTMs to effectively process and remember information over both short and long durations.\n",
    "- The cell state represents the memory of the network, storing information over time\n",
    "    - LSTMs have a complex architecture with forget gates, input gates, and output gates \n",
    "    - The forget gate decides how much of the past information the LSTM should remember\n",
    "    - The input gate decides how much new information to add to the cell state.\n",
    "    - The output gate decides what part of the current cell state makes it to the output.\n",
    "    - These gates control the flow of information in the network\n",
    "    - LSTMs are powerful tools, but they can be difficult to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bidirectional LSTM**\n",
    "- https://www.geeksforgeeks.org/bidirectional-lstm-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Bidirectional LSTM layer Architecture**\n",
    "<div>\n",
    "<img src = \"nlp_images/BidLSTM.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bidirectional LSTMs are a type of recurrent neural network model that is able to process information in both the forward and backward direction \n",
    "- Bidirectional LSTM is a sequence model which contains two LSTM layers, one for processing input in the forward direction and the other for processing in the backward direction. This allows the model to better understand the relationships between words in a sequence\n",
    "- Bidirectional LSTMs are useful for tasks like sentiment analysis, text classification, and machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformers**\n",
    "- https://serokell.io/blog/transformers-in-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Architecture of Transformers**\n",
    "<div>\n",
    "<img src = \"nlp_images/transformer.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "##### This architecture is composed of two main parts: the encoder on the left and the decoder on the right. Each part consists of multiple identical layers (stacked N times as indicated by \"Nx\" in the diagram). Let me explain the key components:\n",
    "\n",
    "- Add & Norm: The outputs of the attention mechanism are added to the original embeddings (residual connection), and then normalized. This helps in stabilizing the learning process.\n",
    "- Feed Forward: Each position passes through a feed-forward neural network that processes the information from the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 3 key elements that make transformers so powerful:\n",
    "    1. Self-attention \n",
    "    2. Positional embeddings \n",
    "    3. Multihead attention\n",
    "    ##### **1. Self-attention**\n",
    "    - Self-attention allows a model to weigh the importance of different words in a sentence relative to each other. It calculates a score indicating how much focus to place on other parts of the input sentence as it processes a word. This mechanism enables the model to capture relationships and dependencies between words, regardless of their positional distance in the sentence. For example, in the sentence \"The cat that sat on the mat was fluffy,\" self-attention helps the model understand that \"cat\" is related to \"fluffy.\"\n",
    "    - The self-attention mechanism enables the model to detect the connection between different elements even if they are far from each other and assess the importance of those connections, therefore, improving the understanding of the context.\n",
    "    ##### **2. Positional Embeddings**\n",
    "    - Positional embeddings are used to give the model information about the order of words in a sentence since models like the Transformer, which rely heavily on attention mechanisms, do not inherently process sequential data in order. \n",
    "    - Positional embeddings are added to the input embeddings and provide a unique signal for each position in the sequence, allowing the model to recognize word order and use this information to better understand the context and meaning of a sentence.\n",
    "    ##### **3. Multihead attention**\n",
    "    - In NLP, sequences (like sentences or documents) of text are typically represented as vectors of word embeddings. However, a word embedding doesn’t capture the position of the word in the sequence.\n",
    "    - Multihead attention is an extension of the attention mechanism that allows the model to focus on different parts of the input sentence for different \"reasons\" or \"aspects.\" Instead of having a single set of attention weights, the model has multiple sets (heads), each capable of focusing on different parts of the input. \n",
    "    - This architecture enables the model to capture a richer array of relationships in the data, as each head can learn to attend to different features of the input. \n",
    "    - Multihead attention combines these diverse perspectives to produce a more comprehensive understanding of the input data. For instance, in one head, the model might focus on the syntactic relationship between words, while another might focus on semantic aspects.\n",
    "\n",
    "Due to positional embeddings and multihead attention, transformers allow for simultaneous sequence processing (parallelization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do **transformers** work?\n",
    "1. Input Embedding\n",
    "    - The transformer starts by converting input tokens (words, characters, etc.) into vectors using embedding layers. These embeddings capture the semantic information of each token.\n",
    "\n",
    "2. Positional Encoding\n",
    "    - Since transformers do not inherently process data in order, positional encodings are added to the input embeddings to provide information about the position of each token in the sequence. This allows the transformer to maintain the sequence's order.\n",
    "\n",
    "3. Self-attention Mechanism\n",
    "    - The self-attention mechanism allows each token to interact with every other token in the input. It computes attention scores to determine how much focus each token should have on every other token, enabling the model to capture relationships between all parts of the input, regardless of their distance.\n",
    "\n",
    "4. Multi-head Attention\n",
    "    - Instead of one set of attention scores, the transformer uses multiple sets or \"heads\" to allow the model to pay attention to different parts of the input for different reasons simultaneously. The outputs of these attention heads are then combined.\n",
    "\n",
    "5. Feed-Forward Neural Networks\n",
    "    - After attention computation, each token's representation is passed through a feed-forward neural network (FFNN). Each token is processed independently, with the same FFNN applied to each position.\n",
    "\n",
    "6. Residual Connections and Layer Normalization\n",
    "    - To help mitigate the vanishing gradient problem and facilitate training of deep networks, residual connections followed by layer normalization are applied after the multi-head attention and FFNN layers.\n",
    "\n",
    "7. Output\n",
    "    - For each token in the input sequence, the transformer outputs a vector. This output can be used for various tasks, such as classification, translation, or generation, depending on the specific configuration of the output layer.\n",
    "\n",
    "8. Decoder (For Encoder-Decoder Architecture)\n",
    "    - In tasks like translation, a decoder similar to the encoder but with additional attention layers to focus on relevant parts of the input sequence is used. The decoder takes the encoder output and generates the target sequence step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Transformers Surpassed RNNs in NLP Performance\n",
    "\n",
    "**Parallel Processing:**\n",
    "\n",
    "- RNN: Difficult to parallelize due to sequential processing.\n",
    "- Transformer: Can process all inputs simultaneously using self-attention mechanisms, making it much faster.\n",
    "\n",
    "\n",
    "**Long-range Dependency Handling:**\n",
    "\n",
    "- RNN: Struggles to connect information from distant parts in long sequences (vanishing gradient problem).\n",
    "- Transformer: Self-attention mechanism allows direct connections between all positions regardless of sequence length.\n",
    "\n",
    "\n",
    "**Fixed Context Size:**\n",
    "\n",
    "- RNN: Must compress information into a fixed-size hidden state.\n",
    "- Transformer: Can dynamically allocate attention to all input tokens.\n",
    "\n",
    "\n",
    "**Positional Information Processing:**\n",
    "\n",
    "- RNN: Naturally includes positional information through sequential processing.\n",
    "- Transformer: Uses explicit positional encoding for more flexible handling of position information.\n",
    "\n",
    "\n",
    "**Computational Efficiency:**\n",
    "\n",
    "- RNN: Computational cost increases with longer sequences.\n",
    "- Transformer: Can process long sequences more efficiently due to parallel processing.\n",
    "\n",
    "\n",
    "**Multi-scale Feature Extraction:**\n",
    "\n",
    "- RNN: Extracts features at a single time scale.\n",
    "- Transformer: Can extract features at various scales simultaneously through multi-head attention.\n",
    "\n",
    "\n",
    "**Model Depth:**\n",
    "\n",
    "- RNN: Difficult to create deep structures.\n",
    "- Transformer: Can effectively train very deep structures through residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                           Sentence  \\\n",
       "0  Sentence: 1  Thousands of demonstrators have marched throug...   \n",
       "1  Sentence: 2  Families of soldiers killed in the conflict jo...   \n",
       "2  Sentence: 3  They marched from the Houses of Parliament to ...   \n",
       "3  Sentence: 4  Police put the number of marchers at 10,000 wh...   \n",
       "4  Sentence: 5  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...   \n",
       "2  ['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...   \n",
       "3  ['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...   \n",
       "4  ['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('TalkFile_ner_2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                           Sentence  \\\n",
       "0  Sentence: 1  Thousands of demonstrators have marched throug...   \n",
       "1  Sentence: 2  Families of soldiers killed in the conflict jo...   \n",
       "2  Sentence: 3  They marched from the Houses of Parliament to ...   \n",
       "3  Sentence: 4  Police put the number of marchers at 10,000 wh...   \n",
       "4  Sentence: 5  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...   \n",
       "2  ['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...   \n",
       "3  ['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...   \n",
       "4  ['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...  \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert these strings into their actual Python data types\n",
    "df['Tag'] = df['Tag'].apply(lambda x: eval(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tag = df['Tag'].to_list()\n",
    "list_tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-tim',\n",
       " 'B-art',\n",
       " 'B-gpe',\n",
       " 'B-per',\n",
       " 'B-org',\n",
       " 'I-per',\n",
       " 'I-gpe',\n",
       " 'B-eve',\n",
       " 'I-org',\n",
       " 'I-eve',\n",
       " 'I-nat',\n",
       " 'I-tim',\n",
       " 'B-geo',\n",
       " 'B-nat',\n",
       " 'I-art',\n",
       " 'I-geo']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "list_labels = ['O'] + [i for i in list(set(chain.from_iterable(list_tag))) if i != 'O']\n",
    "list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-tim',\n",
       " 'B-art',\n",
       " 'B-gpe',\n",
       " 'B-per',\n",
       " 'B-org',\n",
       " 'I-per',\n",
       " 'I-gpe',\n",
       " 'B-eve',\n",
       " 'I-org',\n",
       " 'I-eve',\n",
       " 'I-nat',\n",
       " 'I-tim',\n",
       " 'B-geo',\n",
       " 'B-nat',\n",
       " 'O',\n",
       " 'I-art',\n",
       " 'I-geo']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_labels = ['O'] + [i for tag in list_tag for i in tag if i!='O']\n",
    "list_labels = list(set(list_labels))\n",
    "list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-tim': 0,\n",
       " 'B-art': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'B-org': 4,\n",
       " 'I-per': 5,\n",
       " 'I-gpe': 6,\n",
       " 'B-eve': 7,\n",
       " 'I-org': 8,\n",
       " 'I-eve': 9,\n",
       " 'I-nat': 10,\n",
       " 'I-tim': 11,\n",
       " 'B-geo': 12,\n",
       " 'B-nat': 13,\n",
       " 'O': 14,\n",
       " 'I-art': 15,\n",
       " 'I-geo': 16}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {}\n",
    "for i, v in enumerate(list_labels):\n",
    "    dic[v] = i\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-gpe', 'I-art', 'I-org', 'I-per', 'I-eve', 'I-geo', 'B-geo', 'B-tim', 'B-art', 'I-gpe', 'B-eve', 'B-nat', 'I-nat', 'B-org', 'I-tim', 'B-per']\n"
     ]
    }
   ],
   "source": [
    "#Use the chain.from_iterable class method to flatten a list of lists (or any iterable of iterables)\n",
    "from itertools import chain\n",
    "list_labels = ['O'] + [i for i in list(set(chain.from_iterable(list_tag))) if i != 'O']\n",
    "print(list_labels)\n",
    "#encoding: maps each unique label to a unique integer so that model can understand\n",
    "label_index = {}\n",
    "#decoding: map each index back to its corresponding label, making the predictions understandable\n",
    "index_label = {}\n",
    "for i, l in enumerate(list_labels):\n",
    "    label_index[l] = i\n",
    "    index_label[i] = l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-gpe': 1,\n",
       " 'I-art': 2,\n",
       " 'I-org': 3,\n",
       " 'I-per': 4,\n",
       " 'I-eve': 5,\n",
       " 'I-geo': 6,\n",
       " 'B-geo': 7,\n",
       " 'B-tim': 8,\n",
       " 'B-art': 9,\n",
       " 'I-gpe': 10,\n",
       " 'B-eve': 11,\n",
       " 'B-nat': 12,\n",
       " 'I-nat': 13,\n",
       " 'B-org': 14,\n",
       " 'I-tim': 15,\n",
       " 'B-per': 16}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-gpe',\n",
       " 2: 'I-art',\n",
       " 3: 'I-org',\n",
       " 4: 'I-per',\n",
       " 5: 'I-eve',\n",
       " 6: 'I-geo',\n",
       " 7: 'B-geo',\n",
       " 8: 'B-tim',\n",
       " 9: 'B-art',\n",
       " 10: 'I-gpe',\n",
       " 11: 'B-eve',\n",
       " 12: 'B-nat',\n",
       " 13: 'I-nat',\n",
       " 14: 'B-org',\n",
       " 15: 'I-tim',\n",
       " 16: 'B-per'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 14,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             tokens  \\\n",
       "0   0  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1   1  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2   2  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3   3  [Police, put, the, number, of, marchers, at, 1...   \n",
       "4   4  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 0]  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 14,...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_ind_list = df['Tag'].apply(lambda x: [label_index[i] for i in x]).to_list()\n",
    "text_list = df['Sentence'].apply(lambda x:x.split(' ')).to_list()\n",
    "data_dict = {'id':list(range(len(text_list))),'tokens':text_list,'ner_tags':labels_ind_list}\n",
    "new_df = pd.DataFrame(data_dict)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=17, id2label=index_label, label2id=label_index\n",
    ")\n",
    "model2 = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=17, id2label=index_label, label2id=label_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,test_df = train_test_split(new_df,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/38367 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 38367/38367 [00:02<00:00, 16383.35 examples/s]\n",
      "Map: 100%|██████████| 9592/9592 [00:00<00:00, 17661.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset_dict = datasets.DatasetDict()\n",
    "dataset_dict['train'] = datasets.Dataset.from_pandas(train_df)\n",
    "dataset_dict['test'] = datasets.Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 7707,\n",
       " 'tokens': ['The',\n",
       "  '58-year-old',\n",
       "  'former',\n",
       "  'analyst',\n",
       "  'says',\n",
       "  'he',\n",
       "  'provided',\n",
       "  'information',\n",
       "  'to',\n",
       "  'an',\n",
       "  'official',\n",
       "  'at',\n",
       "  'the',\n",
       "  'Israeli',\n",
       "  'embassy',\n",
       "  'and',\n",
       "  'to',\n",
       "  'two',\n",
       "  'members',\n",
       "  'of',\n",
       "  'a',\n",
       "  'lobbying',\n",
       "  'group',\n",
       "  'called',\n",
       "  'the',\n",
       "  'American',\n",
       "  'Israel',\n",
       "  'Public',\n",
       "  'Affairs',\n",
       "  'Committee',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  14,\n",
       "  3,\n",
       "  3,\n",
       "  0],\n",
       " '__index_level_0__': 7707,\n",
       " 'input_ids': [101,\n",
       "  1996,\n",
       "  5388,\n",
       "  1011,\n",
       "  2095,\n",
       "  1011,\n",
       "  2214,\n",
       "  2280,\n",
       "  12941,\n",
       "  2758,\n",
       "  2002,\n",
       "  3024,\n",
       "  2592,\n",
       "  2000,\n",
       "  2019,\n",
       "  2880,\n",
       "  2012,\n",
       "  1996,\n",
       "  5611,\n",
       "  8408,\n",
       "  1998,\n",
       "  2000,\n",
       "  2048,\n",
       "  2372,\n",
       "  1997,\n",
       "  1037,\n",
       "  19670,\n",
       "  2177,\n",
       "  2170,\n",
       "  1996,\n",
       "  2137,\n",
       "  3956,\n",
       "  2270,\n",
       "  3821,\n",
       "  2837,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  14,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tokenized_dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "import numpy as np\n",
    "\n",
    "labels = [index_label[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [index_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [index_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/4796 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 500/4796 [05:14<44:31,  1.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2909, 'grad_norm': 1.853703498840332, 'learning_rate': 1.791492910758966e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1000/4796 [10:30<43:35,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.14, 'grad_norm': 0.963873565196991, 'learning_rate': 1.5829858215179316e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 1500/4796 [15:51<33:07,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1275, 'grad_norm': 0.8642410635948181, 'learning_rate': 1.3744787322768976e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2000/4796 [21:18<27:30,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1179, 'grad_norm': 0.8836368322372437, 'learning_rate': 1.1659716430358635e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2398/4796 [25:44<26:57,  1.48it/s]C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                   \n",
      " 50%|█████     | 2398/4796 [27:28<26:57,  1.48it/s]Checkpoint destination directory .\\checkpoint-2398 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11251270025968552, 'eval_precision': 0.8030616284696257, 'eval_recall': 0.815509693558474, 'eval_f1': 0.809237793390811, 'eval_accuracy': 0.9658309880726743, 'eval_runtime': 103.603, 'eval_samples_per_second': 92.584, 'eval_steps_per_second': 5.791, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2500/4796 [28:36<25:03,  1.53it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.112, 'grad_norm': 0.8961143493652344, 'learning_rate': 9.57464553794829e-06, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3000/4796 [34:06<19:15,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0989, 'grad_norm': 0.9021720886230469, 'learning_rate': 7.4895746455379494e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 3500/4796 [39:32<14:20,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0983, 'grad_norm': 0.9885796904563904, 'learning_rate': 5.404503753127606e-06, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4000/4796 [44:59<07:41,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0979, 'grad_norm': 0.9366612434387207, 'learning_rate': 3.319432860717265e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 4500/4796 [50:20<03:02,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0918, 'grad_norm': 0.7471380233764648, 'learning_rate': 1.2343619683069227e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4796/4796 [53:32<00:00,  1.39it/s]C:\\Users\\alice\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                   \n",
      "100%|██████████| 4796/4796 [55:14<00:00,  1.39it/s]Checkpoint destination directory .\\checkpoint-4796 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10554414987564087, 'eval_precision': 0.8175175977902521, 'eval_recall': 0.8197087465380148, 'eval_f1': 0.8186117059243398, 'eval_accuracy': 0.967841211494649, 'eval_runtime': 102.1942, 'eval_samples_per_second': 93.861, 'eval_steps_per_second': 5.871, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4796/4796 [55:15<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3315.332, 'train_samples_per_second': 23.145, 'train_steps_per_second': 1.447, 'train_loss': 0.12816107044426772, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4796, training_loss=0.12816107044426772, metrics={'train_runtime': 3315.332, 'train_samples_per_second': 23.145, 'train_steps_per_second': 1.447, 'train_loss': 0.12816107044426772, 'epoch': 2.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "text = ' '.join(tokenized_dataset[\"test\"]['tokens'][0])\n",
    "print(text)\n",
    "classifier = pipeline(\"ner\", model=model,tokenizer=tokenizer)\n",
    "classifier(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
