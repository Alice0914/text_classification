{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>ID</th>\n",
       "      <th>SPECIES</th>\n",
       "      <th>BREED ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abyssinian_100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abyssinian_101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abyssinian_102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abyssinian_103</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abyssinian_104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Image  ID  SPECIES  BREED ID\n",
       "0  Abyssinian_100   1        1         1\n",
       "1  Abyssinian_101   1        1         1\n",
       "2  Abyssinian_102   1        1         1\n",
       "3  Abyssinian_103   1        1         1\n",
       "4  Abyssinian_104   1        1         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(f\"annotations/trainval.txt\", sep=\" \", \n",
    "                      names = [\"Image\", \"ID\", \"SPECIES\", \"BREED ID\"])\n",
    "dataset.reset_index(drop=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Abyssinian',\n",
       " 1: 'American Bulldog',\n",
       " 2: 'American Pit Bull Terrier',\n",
       " 3: 'Basset Hound',\n",
       " 4: 'Beagle',\n",
       " 5: 'Bengal',\n",
       " 6: 'Birman',\n",
       " 7: 'Bombay',\n",
       " 8: 'Boxer',\n",
       " 9: 'British Shorthair',\n",
       " 10: 'Chihuahua',\n",
       " 11: 'Egyptian Mau',\n",
       " 12: 'English Cocker Spaniel',\n",
       " 13: 'English Setter',\n",
       " 14: 'German Shorthaired',\n",
       " 15: 'Great Pyrenees',\n",
       " 16: 'Havanese',\n",
       " 17: 'Japanese Chin',\n",
       " 18: 'Keeshond',\n",
       " 19: 'Leonberger',\n",
       " 20: 'Maine Coon',\n",
       " 21: 'Miniature Pinscher',\n",
       " 22: 'Newfoundland',\n",
       " 23: 'Persian',\n",
       " 24: 'Pomeranian',\n",
       " 25: 'Pug',\n",
       " 26: 'Ragdoll',\n",
       " 27: 'Russian Blue',\n",
       " 28: 'Saint Bernard',\n",
       " 29: 'Samoyed',\n",
       " 30: 'Scottish Terrier',\n",
       " 31: 'Shiba Inu',\n",
       " 32: 'Siamese',\n",
       " 33: 'Sphynx',\n",
       " 34: 'Staffordshire Bull Terrier',\n",
       " 35: 'Wheaten Terrier',\n",
       " 36: 'Yorkshire Terrier'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating mappings between class labels and breeds\n",
    "image_ids = []\n",
    "labels = []\n",
    "with open(f\"annotations/trainval.txt\") as file:\n",
    "    for line in file:\n",
    "        image_id, label, *_ = line.strip().split()\n",
    "        image_ids.append(image_id)\n",
    "        labels.append(int(label)-1)\n",
    "\n",
    "classes = [\n",
    "    \" \".join(part.title() for part in raw_cls.split(\"_\"))\n",
    "    for raw_cls, _ in sorted(\n",
    "        {(image_id.rsplit(\"_\", 1)[0], label) for image_id, label in zip(image_ids, labels)},\n",
    "        key=lambda image_id_and_label: image_id_and_label[1],\n",
    "    )\n",
    "    ]\n",
    "idx_to_class = dict(zip(range(len(classes)), classes))\n",
    "idx_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>ID</th>\n",
       "      <th>SPECIES</th>\n",
       "      <th>BREED ID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abyssinian_100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Abyssinian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abyssinian_101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Abyssinian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abyssinian_102</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Abyssinian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abyssinian_103</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Abyssinian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abyssinian_104</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Abyssinian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Image  ID  SPECIES  BREED ID       class\n",
       "0  Abyssinian_100   0        1         1  Abyssinian\n",
       "1  Abyssinian_101   0        1         1  Abyssinian\n",
       "2  Abyssinian_102   0        1         1  Abyssinian\n",
       "3  Abyssinian_103   0        1         1  Abyssinian\n",
       "4  Abyssinian_104   0        1         1  Abyssinian"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['ID'] = dataset['ID'] - 1\n",
    "\n",
    "decode_map = idx_to_class\n",
    "def decode_label(label):\n",
    "    return decode_map[int(label)]\n",
    "\n",
    "dataset[\"class\"] = dataset[\"ID\"].apply(lambda x: decode_label(x))\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Abyssinian                    100\n",
       "Russian Blue                  100\n",
       "Maine Coon                    100\n",
       "Miniature Pinscher            100\n",
       "Persian                       100\n",
       "Pomeranian                    100\n",
       "Pug                           100\n",
       "Ragdoll                       100\n",
       "Saint Bernard                 100\n",
       "American Bulldog              100\n",
       "Samoyed                       100\n",
       "Scottish Terrier              100\n",
       "Shiba Inu                     100\n",
       "Sphynx                        100\n",
       "Staffordshire Bull Terrier    100\n",
       "Wheaten Terrier               100\n",
       "Leonberger                    100\n",
       "Keeshond                      100\n",
       "Japanese Chin                 100\n",
       "Havanese                      100\n",
       "American Pit Bull Terrier     100\n",
       "Basset Hound                  100\n",
       "Beagle                        100\n",
       "Bengal                        100\n",
       "Birman                        100\n",
       "Boxer                         100\n",
       "British Shorthair             100\n",
       "Chihuahua                     100\n",
       "English Setter                100\n",
       "German Shorthaired            100\n",
       "Great Pyrenees                100\n",
       "Yorkshire Terrier             100\n",
       "Siamese                        99\n",
       "Bombay                         96\n",
       "English Cocker Spaniel         96\n",
       "Newfoundland                   96\n",
       "Egyptian Mau                   93\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = dataset['class']\n",
    "x = dataset['Image']\n",
    "\n",
    "trainval, x_test, y_trainval, y_test = train_test_split(x, y,\n",
    "                                                        stratify=y, \n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(  trainval, y_trainval,\n",
    "                                                    stratify=y_trainval, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_directory = os.path.join(\"images\", \"images\")\n",
    "masks_directory = os.path.join(\"annotations\", \"trimaps\") #includ pixel values of a mask image\n",
    "\n",
    "train_images_filenames = x_train.reset_index(drop=True)\n",
    "val_images_filenames = x_val.reset_index(drop=True)\n",
    "test_images_filenames = x_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None, transform_mask=None):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "        self.transform = transform\n",
    "        self.transform_mask = transform_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames.loc[idx] + '.jpg' \n",
    "        image = Image.open(os.path.join(self.images_directory, image_filename)).convert('RGB')\n",
    "        mask = Image.open(\n",
    "            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")))\n",
    "        #mask = preprocess_mask(mask)\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image)\n",
    "            transformed_m = self.transform_mask(mask)\n",
    "            image = transformed\n",
    "            mask = transformed_m\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Training Data\n",
    "train_transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                transforms.ToTensor(), # [0, 255] -> [0.0, 1.0]\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #(pixel-mean)/std => [0.0, 1.0] ->  [-1.0, 1.0] \n",
    "#For Class Label Image: Label images are used in segmentation tasks to indicate the class of the object to which each pixel belongs.\n",
    "target_transform = transforms.Compose([transforms.PILToTensor(),  #Maintains the original pixel value range  \n",
    "                                       transforms.Resize((256, 256)),   \n",
    "                                       # Start Class Label from 0, Dimention Reduction, Label Data Type Change  \n",
    "                                       transforms.Lambda(lambda x: (x-1).squeeze().type(torch.LongTensor)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OxfordPetDataset(train_images_filenames, \n",
    "                                 images_directory, \n",
    "                                 masks_directory, \n",
    "                                 transform=train_transform, \n",
    "                                 transform_mask=target_transform)\n",
    "\n",
    "\n",
    "val_dataset = OxfordPetDataset(val_images_filenames,\n",
    "                               images_directory,\n",
    "                               masks_directory,\n",
    "                               transform=train_transform,\n",
    "                               transform_mask=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alice\\Documents\\NLP\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alice\\Documents\\NLP\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alice\\Documents\\NLP\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\alice/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:04<00:00, 25.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained ResNet50 model with an FPN backbone.\n",
    "    backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
    "    model = FasterRCNN(backbone, num_classes=num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assuming two classes: 1 for pet face + 1 for background.\n",
    "num_classes = 2\n",
    "model = get_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class PetFaceDataset(Dataset):\n",
    "    def __init__(self, annotations_file, images_dir, transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming img_path and bounding boxes are correctly loaded here\n",
    "        img_path = os.path.join(self.images_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load your bounding box annotations here\n",
    "        box = self.img_labels.iloc[idx, 1:5].values\n",
    "        box = torch.tensor(box, dtype=torch.float32)\n",
    "        \n",
    "        # Ensure labels are loaded or defined here, for simplicity assuming label is 1 for all\n",
    "        labels = torch.tensor([1], dtype=torch.int64)  # Assuming all objects are 'pet face'\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = box.unsqueeze(0)  # Model expects boxes in a list of tensors\n",
    "        target[\"labels\"] = labels\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Transformation pipeline\n",
    "transforms = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alice\\Documents\\NLP\\venv\\Lib\\site-packages\\torch\\nn\\init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def create_model(num_classes):\n",
    "    # Load a model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assuming 2 classes: 1 for pet face + 1 for background\n",
    "model = create_model(num_classes=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     27\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m---> 28\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     30\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n\u001b[0;32m     31\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Assuming the dataset has been instantiated as `train_dataset`\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Move model to the right device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 3\n",
    "lr = 0.005\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)  # Example scheduler\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_dataloader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch: {epoch+1}, Average Loss: {avg_loss}\")\n",
    "    scheduler.step()  # Adjust learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('f', 'f', 'f'), ('l', 'l', 'l'), ('o', 'o', 'i'), ('w', 'w', 'g')]\n"
     ]
    }
   ],
   "source": [
    "strs = [\"flower\",\"flow\",\"flight\"]\n",
    "list_s = list(zip(*strs))\n",
    "print(list_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f', 'f', 'f')\n",
      "('l', 'l', 'l')\n",
      "('o', 'o', 'i')\n",
      "('w', 'w', 'g')\n"
     ]
    }
   ],
   "source": [
    "strs = [\"flower\",\"flow\",\"flight\"]\n",
    "for i in zip(*strs):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]\n"
     ]
    }
   ],
   "source": [
    "s=\"()[]{}\"\n",
    "print(s[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_id = ms_project['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "pd.sorted_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
